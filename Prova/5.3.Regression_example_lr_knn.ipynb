{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Example with Sklearn\n",
    "\n",
    "With Linear Regression and kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora he il momento di di isolare la colonna target e omettere colonna id e split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Choose a range of values for the number of neighbors (e.g., from 1 to 30).\n",
    " ‚ó¶ Apply cross-validation with the kNN model across this range (using a loop), storing the\n",
    " performance for each different number of neighbors. Set other hyperparameters as default,\n",
    " except \n",
    "n_jobs, which you can set to -1 to speed up computation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ó¶ Plot the training and validation performances as a function of the different number of\n",
    " neighbors. For example, use the x-axis for the number of neighbors and the y-axis for the\n",
    " performance metric. You can also use different colors for training and validation data. The\n",
    " resulting plot should be something similar to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression now without scaling check the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn now without scaling check the performance - noterai che il modello va molto peggio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Escalador | Train MSE | Validation MSE | Train MAE | Validation MAE | C√≥mo funciona |\n",
    "|------------|------------|----------------|------------|----------------|----------------|\n",
    "| **MinMaxScaler** | 244,544,681.9 | 430,381,109.6 | 10,875.7 | 14,512.3 | Reescala todas las variables entre 0 y 1. Mantiene la forma de la distribuci√≥n original, pero cambia la escala. Ideal para KNN y modelos basados en distancia. |\n",
    "| **RobustScaler** | 347,861,372.1 | 595,551,691.9 | 11,830.3 | 15,664.1 | Reescala usando mediana y rango intercuart√≠lico (IQR) en lugar de media y desviaci√≥n est√°ndar. Es m√°s robusto frente a valores at√≠picos. |\n",
    "| **StandardScaler** | 211,912,658.5 | 360,117,245.3 | 10,344.9 | 13,507.4 | Estandariza los datos a media 0 y desviaci√≥n est√°ndar 1. Supone que las variables siguen una distribuci√≥n normal. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOISE\n",
    "\n",
    "Perform an experiment by adding noisy (random) features (columns) to your training data,\n",
    " while keeping all original features intact. Create a line plot where:\n",
    " ‚ó¶ The x-axis shows the number of noisy fts added, from 0 (no noise) to 100, in steps of 10.\n",
    " ‚ó¶ The y-axis shows the performance metric of your choice.\n",
    " ‚ó¶ What are your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una copia con ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el modelo con ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset                         | Train MSE     | Validation MSE | Train MAE | Validation MAE |\n",
    "| ------------------------------- | ------------- | -------------- | --------- | -------------- |\n",
    "| **Sin ruido (`X_train`)**       | 211,912,658.5 | 360,117,245.3  | 10,344.9  | 13,507.4       |\n",
    "| **Con ruido (`X_train_noisy`)** | 236,526,818.0 | 396,896,801.5  | 10,917.2  | 14,285.9       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weights  hyperparameter in kNN determines   \n",
    " how much influence each neighbor has on the\n",
    " predicted output.\n",
    " By default, kNN assigns equal weight to all neighbors (\n",
    " When you set \n",
    " weights='uniform' ).\n",
    " weights='distance' , closer neighbors have a stronger impact on the prediction\n",
    " than those further away. This can lead to improved performance, especially in datasets where\n",
    " proximity is more indicative of the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Par√°metro `weights` | Train MSE | Validation MSE | Train MAE | Validation MAE | C√≥mo funciona |\n",
    "|----------------------|-----------|----------------|------------|----------------|----------------|\n",
    "| **distance** | 105,088.8 | 347,105,123.9 | 29.1 | 13,114.8 | Los vecinos **m√°s cercanos tienen m√°s peso** en la predicci√≥n. Esto puede mejorar la precisi√≥n, pero tambi√©n hace que el modelo se adapte m√°s a los datos de entrenamiento (mayor riesgo de sobreajuste). |\n",
    "| **uniform** | 211,912,658.5 | 360,117,245.3 | 10,344.9 | 13,507.4 | Todos los vecinos tienen **el mismo peso**, sin importar su distancia. Es m√°s estable y menos sensible al ruido, pero puede ser menos preciso en regiones con alta variaci√≥n local. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan distance VS  Euclidean distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The hyperparameter \n",
    "p  determines the distance metric used by kNN. \n",
    "Manhattan distance, \n",
    "p=1  corresponds to the\n",
    " p=2  corresponds to the Euclidean distance, and higher values of \n",
    "p  are\n",
    " different (more abstract) distance calculations, but still valid.\n",
    " ‚ó¶ Create a heatmap to visualize the performance of kNN with various combinations of\n",
    " n_neighbors (e.g., 1 to 10) and \n",
    "p  values (e.g., 1 to 5). Each cell in the heatmap should\n",
    " represent the performance metric for a specific combination of \n",
    "color of each cell summarizes the performance\n",
    "\n",
    "\n",
    "P es el parametro de la distancia de meincosky\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| p (Minkowski) | weights  | Train MSE     | Validation MSE | Train MAE | Validation MAE |\n",
    "|---------------|----------|---------------|----------------|-----------|----------------|\n",
    "| 1 (Manhattan) | uniform  | 178,027,560.9 | 302,798,322.3  | 9,402.9   | 12,298.3       |\n",
    "| 2 (Euclidean) | uniform  | 211,912,658.5 | 360,117,245.3  | 10,344.9  | 13,507.4       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression: Tree-based Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Decision Tree (DT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a range for the maximum tree depth.\n",
    "\n",
    "        ‚ó¶ Start with a low depth value (e.g., 1) and finish with `None` (deepest   tree possible). Set other parameters as default.\n",
    "        ‚ó¶ Plot a chart with training and validation scores based on these different values.\n",
    "        ‚ó¶ What do you think is the best maximum depth?\n",
    "\n",
    " ‚Ä¢ Perform the same study with the minimum samples split.\n",
    "\n",
    " ‚Ä¢ Perform the same study with the minimum samples leaf.\n",
    "\n",
    " ‚Ä¢ Compare performance when using different criterion.\n",
    "\n",
    " ‚Ä¢ Train a DT with low depth (e.g., 3 or 4) and plot it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a range for the maximum tree depth.\n",
    " ‚ó¶ Start with a low depth value (e.g., 1) and finish with `None` (deepest tree possible). Set \n",
    "other parameters as default.\n",
    " ‚ó¶ Plot a chart with training and validation scores based on these different values.\n",
    " ‚ó¶ What do you think is the best maximum depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same study with the minimum samples split \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Ä¢ Perform the same study with the minimum samples leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compare performance when using different \n",
    " criterion ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il parametro criterion dice al Decision Tree come scegliere la miglior divisione possibile in ogni nodo.\n",
    "\n",
    "\n",
    "\n",
    "| Criterion          | Significato                                                                              | Quando usarlo                 |\n",
    "| ------------------ | ---------------------------------------------------------------------------------------- | ----------------------------- |\n",
    "| `'squared_error'`  | Minimizza la somma dei quadrati degli errori ‚Üí come il **RMSE**                          | Default, buono in generale    |\n",
    "| `'absolute_error'` | Minimizza la somma degli errori assoluti ‚Üí come il **MAE**, pi√π **robusto agli outlier** | Quando ci sono valori estremi |\n",
    "| `'friedman_mse'`   | Variante ottimizzata per **gradient boosting** (pi√π efficiente nei modelli ensemble)     | Raramente utile da solo       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come si interpreta:\n",
    "\n",
    "Guardi il Validation RMSE pi√π basso ‚Üí quello indica il criterio migliore.\n",
    "\n",
    "Se i valori sono simili, puoi scegliere il default (squared_error).\n",
    "\n",
    "Se hai molti outlier, absolute_error pu√≤ dare risultati pi√π stabili."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest (RF)\n",
    " ‚Ä¢ Choose a range for the number of estimators.\n",
    " ‚ó¶ Start with a low number and finish with 2,000. Set other parameters as default. Do not try \n",
    "all numbers up to 2,000, it will take a lot of time! Try, for example, [1, 10, 50, 100, 300, ‚Ä¶].\n",
    " ‚ó¶ Plot a chart with training and validation scores based on these different values.\n",
    " ‚ó¶ What do you think is the best number of estimators?\n",
    " ‚Ä¢ Perform the same study with the number of maximum features. Try options like \"sqrt\", \"log2\", \n",
    "or even a fraction of the total features.\n",
    " ‚Ä¢ Experiment with using bootstrap samples (default) vs. the whole dataset for building each tree.\n",
    " ‚Ä¢ Learn about RF and feature importances. Analyze the importance of each feature in your \n",
    "dataset. Which are the most important ones? And the least important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Fase     | n_estimators | Cosa succede                         | Interpretazione                          |\n",
    "| -------- | ------------ | ------------------------------------ | ---------------------------------------- |\n",
    "| Inizio   | 1‚Äì10         | Errori alti e instabili              | Troppo pochi alberi ‚Üí modello debole     |\n",
    "| Crescita | 50‚Äì300       | Errori scendono e si stabilizzano    | Ottimo compromesso                       |\n",
    "| Oltre    | >500         | Errori stabili, nessun miglioramento | Pi√π alberi = pi√π tempo, stesso risultato |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora passiamo allo studio del parametro max_features della Random Forest Regressor, usando MAE come metrica.\n",
    "\n",
    "\n",
    "| Valore            | Significato                      | Effetto                                       |\n",
    "| ----------------- | -------------------------------- | --------------------------------------------- |\n",
    "| `\"sqrt\"`          | usa ‚àö(n_features) per ogni split | standard per regressione, buon equilibrio     |\n",
    "| `\"log2\"`          | usa log‚ÇÇ(n_features)             | pi√π casualit√†, minore correlazione tra alberi |\n",
    "| `None`            | usa tutte le feature             | alberi molto simili ‚Üí rischio di overfitting  |\n",
    "| `0.5` (fractions) | usa met√† delle feature           | personalizzabile, utile per tuning fine       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| max_features | Train MAE         | Validation MAE                | Interpretazione                              |\n",
    "| ------------ | ----------------- | ----------------------------- | -------------------------------------------- |\n",
    "| `\"sqrt\"`     | alto (~5000)      | alto (~10700)                 | modello usa poche feature ‚Üí **underfitting** |\n",
    "| `\"log2\"`     | simile a `\"sqrt\"` | leggermente peggio            | ancora **underfitting**                      |\n",
    "| `None`       | pi√π basso (~4200) | migliora (~10400)             | pi√π feature ‚Üí **meno bias**, apprende di pi√π |\n",
    "| `0.5`        | basso (~4300)     | leggermente migliore (~10160) | **miglior compromesso**                      |\n",
    "| `0.8`        | simile a `0.5`    | un po‚Äô peggiore               | troppa libert√†, alberi pi√π simili            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Come leggerlo\n",
    "\n",
    "Il Train MAE scende (cio√® l‚Äôerrore si riduce) man mano che permetti agli alberi di usare pi√π feature ‚Üí normale, perch√© il modello diventa pi√π potente.\n",
    "\n",
    "Il Validation MAE scende fino a un punto ottimale (attorno a max_features = 0.5), poi non migliora pi√π o peggiora leggermente.\n",
    "\n",
    "üëâ Quindi il modello con max_features = 0.5:\n",
    "\n",
    "Ha abbastanza libert√† per imparare,\n",
    "\n",
    "Ma mantiene diversit√† tra alberi,\n",
    "\n",
    "E generalizza meglio ai nuovi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Experiment with using bootstrap samples (default) vs. the whole dataset for building each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Gradient Boosted Decision Trees (GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Gradient Boosted Decision Trees (GBDT)\n",
    " ‚Ä¢ Choose a range for the learning rate.\n",
    " ‚ó¶ Start with a low value (e.g., 0.001) and finish with 1. Set other parameters as default.\n",
    " ‚ó¶ Plot a chart with training and validation scores based on these different values.\n",
    " ‚ó¶ What do you think is the best learning rate?\n",
    " ‚Ä¢ Perform the same study with the tree depth.\n",
    " ‚Ä¢ Experiment with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Ä¢ Choose a range for the learning rate. ‚ó¶ Start with a low value (e.g., 0.001) and finish with 1. Set other parameters as default."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
